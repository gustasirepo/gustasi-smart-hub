# robots.txt for Gustasi - Restaurant POS System
# Last Updated: 2025-09-09

# Global rules for all crawlers
User-agent: *
Allow: /
Allow: /$
Allow: /sitemap.xml$
Allow: /*.css$
Allow: /*.js$
Allow: /*.png$
Allow: /*.jpg$
Allow: /*.jpeg$
Allow: /*.gif$
Allow: /*.svg$
Allow: /*.webp$

# Disallow admin and private areas
Disallow: /private/
Disallow: /admin/
Disallow: /api/
Disallow: /dashboard/
Disallow: /account/
Disallow: /login/
Disallow: /signup/
Disallow: /checkout/

# Disallow query parameters that may cause duplicate content
Disallow: /*?*
Disallow: /*&*
Disallow: /*=*

# Disallow specific file types
Disallow: /*.json$
Disallow: /*.xml$
Disallow: /*.php$
Disallow: /*.aspx$
Disallow: /*.jsp$

# Disallow development and staging environments
Disallow: /*.env
Disallow: /*.git
Disallow: /*.gitignore
Disallow: /*.htaccess
Disallow: /*.htpasswd
Disallow: /*.DS_Store

# Crawl-delay for aggressive crawlers (requests per second)
# Adjust based on your server capacity
Crawl-delay: 10

# Sitemap location (absolute URL)
Sitemap: https://www.gustasi.com/sitemap.xml

# ===== Specific Crawler Rules =====

# Googlebot
# Clean-param: ref /products/
# Clean-param: sessionid /products/

# Allow Googlebot-Image to index all images
User-agent: Googlebot-Image
Allow: /*
Disallow: 

# Additional rules for specific search engines
User-agent: Googlebot
Allow: /*
Disallow: /private/
Disallow: /admin/
Disallow: /api/

User-agent: Bingbot
Allow: /*
Disallow: /private/
Disallow: /admin/
Disallow: /api/

# Block AI data scrapers
User-agent: CCBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: GPTBot
Disallow: /

# Development and staging environments
User-agent: *
Disallow: /staging/
Disallow: /dev/
Disallow: /test/

# Crawl stats
# Visit https://www.gustasi.com/search-console for crawl stats
